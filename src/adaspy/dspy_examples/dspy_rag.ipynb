{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinvegda/Desktop/code/ADASPy/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kevinvegda/Desktop/code/ADASPy/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:112: UserWarning: Protobuf gencode version 5.27.2 is older than the runtime version 5.28.0 at grpc_health/v1/health.proto. Please avoid checked-in Protobuf gencode that can be obsolete.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "  \n",
    "WCS_API_KEY = os.getenv(\"WCS_API_KEY\")\n",
    "WEAVIATE_CLUSTER_URL= os.getenv(\"WEAVIATE_CLUSTER_URL\")\n",
    "  \n",
    "# Connect to a WCS instance\n",
    "weaviate_client = weaviate.connect_to_wcs(\n",
    "    cluster_url=WEAVIATE_CLUSTER_URL,\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(WCS_API_KEY),\n",
    "    headers = {\n",
    "        'X-Openai-Api-Key': os.getenv(\"OPENAI_API_KEY\")\n",
    "    }\n",
    "    )\n",
    "\n",
    "llm = dspy.OpenAI(model = \"gpt-4o-mini\")\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client)\n",
    "\n",
    "dspy.settings.configure(lm = llm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In layers deep, connections weave,  \\nPatterns dance, as data grieves,  \\nA mind of code, where thoughts conceive.']\n",
      "[\"In tangled webs of code they weave,\\nMimicking the mind's intricate sieve,\\nNeural dreams in silicon conceive.\"]\n"
     ]
    }
   ],
   "source": [
    "print(dspy.settings.lm(\"Write a 3 line poem about neural networks.\"))\n",
    "context_example = dspy.OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "with dspy.context(llm=context_example):\n",
    "    print(context_example(\"Write a 3 line poem about neural networks.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why would I use Weaviate as my vector database?',\n",
       " 'What is the difference between Weaviate and for example Elasticsearch?',\n",
       " 'Do you offer Weaviate as a managed service?',\n",
       " 'How should I configure the size of my instance?',\n",
       " 'Do I need to know about Docker (Compose) to use Weaviate?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "f = open(\"faq.md\")\n",
    "markdown_content = f.read()\n",
    "\n",
    "def parse_question(markdown_content):\n",
    "    question_pattern = r'#### Q: (.+?)\\n'\n",
    "    questions = re.findall(question_pattern, markdown_content, re.DOTALL)\n",
    "    return questions\n",
    "\n",
    "questions = parse_question(markdown_content)\n",
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset =  questions[:20]\n",
    "devset = questions[20:30]\n",
    "testset = questions[30:]\n",
    "\n",
    "trainset = [dspy.Example(question=question).with_inputs(\"question\") for question in trainset]\n",
    "devset = [dspy.Example(question=question).with_inputs(\"question\") for question in devset]\n",
    "testset = [dspy.Example(question=question).with_inputs(\"question\") for question in testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricLM = dspy.OpenAI(model = 'gpt-4o', max_tokens = 1000, model_type='chat')\n",
    "\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the quality of an answer to a question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc = \"The context for answering the question\")\n",
    "    assessment_criterion = dspy.InputField(desc = \"The evaluation criterion\")\n",
    "    assessed_answer = dspy.InputField(desc = \"The answer to the question\")\n",
    "    assessment_answer = dspy.OutputField(desc = \"A rating between 1 and 5. Only output the rating and nothing else.\")\n",
    "\n",
    "def llm_metric(gold, pred, trace = None):\n",
    "    predicted_answer = pred.answer\n",
    "    question = gold.question\n",
    "\n",
    "    print(f\"Test question: {question}\")\n",
    "    print(f\"Predicted answer: {predicted_answer}\")\n",
    "\n",
    "    detail = \"Is the assessed answer detailed?\"\n",
    "    faithful = \"Is the assessed text grounded in the context? Say no if it includes significant information not in the context.\"\n",
    "    overall = f\"Please rate how well this answer addresses the question, `{question}` based on the context.\\n `{predicted_answer}`\"\n",
    "\n",
    "    with dspy.context(lm = metricLM):\n",
    "        context = dspy.Retrieve(k = 5)(question).passages\n",
    "        detail = dspy.ChainOfThought(Assess)(context = 'N/A', assessment_criterion = detail, assessed_answer = predicted_answer)\n",
    "        faithful = dspy.ChainOfThought(Assess)(context = context, assessment_criterion = faithful, assessed_answer = predicted_answer)\n",
    "        overall = dspy.ChainOfThought(Assess)(context = context, assessment_criterion = overall, assessed_answer = predicted_answer)\n",
    "\n",
    "    print(f\"Faithful: {faithful.assessment_answer}\")\n",
    "    print(f\"Detail: {detail.assessment_answer}\")\n",
    "    print(f\"Overall: {overall.assessment_answer}\")\n",
    "\n",
    "    total = float(detail.assessment_answer) + float(faithful.assessment_answer) * 2 + float(overall.assessment_answer)\n",
    "\n",
    "    return total / 5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: What do cross encoders do?\n",
      "Predicted answer: They re-rank documents.\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = dspy.Example(question = \"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer = \"They re-rank documents.\")\n",
    "\n",
    "llm_metric(test_example, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: What do cross encoders do?\n",
      "Predicted answer: They index data.\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = dspy.Example(question=\"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer=\"They index data.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question\n",
      "\n",
      "Assessment Criterion: The evaluation criterion\n",
      "\n",
      "Assessed Answer: The answer to the question\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.Â»\n",
      "[2] Â«Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.Â»\n",
      "[3] Â«A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.Â»\n",
      "[4] Â«![Cross-Encoder](./img/cross-encoder.png)\n",
      "\n",
      "*Figure 3 - Representation of a Cross-Encoder model*\n",
      "\n",
      "\n",
      "If a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application â with thousands or millions of objects â this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\n",
      "\n",
      "We can combine the two methods to benefit from the strong points of both models! I'd like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.Â»\n",
      "[5] Â«To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\n",
      "\n",
      "We can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.Â»\n",
      "\n",
      "Assessment Criterion: Is the assessed text grounded in the context? Say no if it includes significant information not in the context.\n",
      "\n",
      "Assessed Answer: They index data.\n",
      "\n",
      "Reasoning: Let's think step by step in order to 1\n",
      "\n",
      "Assessment Answer:\u001b[32m 1\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question\n",
      "\n",
      "Assessment Criterion: The evaluation criterion\n",
      "\n",
      "Assessed Answer: The answer to the question\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.Â»\n",
      "[2] Â«Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.Â»\n",
      "[3] Â«A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.Â»\n",
      "[4] Â«![Cross-Encoder](./img/cross-encoder.png)\n",
      "\n",
      "*Figure 3 - Representation of a Cross-Encoder model*\n",
      "\n",
      "\n",
      "If a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application â with thousands or millions of objects â this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\n",
      "\n",
      "We can combine the two methods to benefit from the strong points of both models! I'd like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.Â»\n",
      "[5] Â«To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\n",
      "\n",
      "We can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.Â»\n",
      "\n",
      "Assessment Criterion: Please rate how well this answer addresses the question, `What do cross encoders do?` based on the context. `They index data.`\n",
      "\n",
      "Assessed Answer: They index data.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Assessment Answer: 1\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question\n",
      "\n",
      "Assessment Criterion: The evaluation criterion\n",
      "\n",
      "Assessed Answer: The answer to the question\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.Â»\n",
      "[2] Â«Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.Â»\n",
      "[3] Â«A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.Â»\n",
      "[4] Â«![Cross-Encoder](./img/cross-encoder.png)\n",
      "\n",
      "*Figure 3 - Representation of a Cross-Encoder model*\n",
      "\n",
      "\n",
      "If a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application â with thousands or millions of objects â this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\n",
      "\n",
      "We can combine the two methods to benefit from the strong points of both models! I'd like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.Â»\n",
      "[5] Â«To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\n",
      "\n",
      "We can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.Â»\n",
      "\n",
      "Assessment Criterion: Please rate how well this answer addresses the question, `What do cross encoders do?` based on the context. `They index data.`\n",
      "\n",
      "Assessed Answer: They index data.\n",
      "\n",
      "Reasoning: Let's think step by step in order to Assessment Answer: 1\n",
      "\n",
      "Assessment Answer:\u001b[32m 1\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question\\n\\nAssessment Criterion: The evaluation criterion\\n\\nAssessed Answer: The answer to the question\\n\\nReasoning: Let\\'s think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] Â«[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.Â»\\n[2] Â«Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.Â»\\n[3] Â«A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.Â»\\n[4] Â«![Cross-Encoder](./img/cross-encoder.png)\\n\\n*Figure 3 - Representation of a Cross-Encoder model*\\n\\n\\nIf a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application â with thousands or millions of objects â this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\\n\\nWe can combine the two methods to benefit from the strong points of both models! I\\'d like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.Â»\\n[5] Â«To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\\n\\nWe can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.Â»\\n\\nAssessment Criterion: Is the assessed text grounded in the context? Say no if it includes significant information not in the context.\\n\\nAssessed Answer: They index data.\\n\\nReasoning: Let\\'s think step by step in order to 1\\n\\nAssessment Answer:\\x1b[32m 1\\x1b[0m\\n\\n\\n\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question\\n\\nAssessment Criterion: The evaluation criterion\\n\\nAssessed Answer: The answer to the question\\n\\nReasoning: Let\\'s think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] Â«[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.Â»\\n[2] Â«Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.Â»\\n[3] Â«A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.Â»\\n[4] Â«![Cross-Encoder](./img/cross-encoder.png)\\n\\n*Figure 3 - Representation of a Cross-Encoder model*\\n\\n\\nIf a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application â with thousands or millions of objects â this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\\n\\nWe can combine the two methods to benefit from the strong points of both models! I\\'d like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.Â»\\n[5] Â«To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\\n\\nWe can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.Â»\\n\\nAssessment Criterion: Please rate how well this answer addresses the question, `What do cross encoders do?` based on the context. `They index data.`\\n\\nAssessed Answer: They index data.\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32m Assessment Answer: 1\\x1b[0m\\n\\n\\n\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question\\n\\nAssessment Criterion: The evaluation criterion\\n\\nAssessed Answer: The answer to the question\\n\\nReasoning: Let\\'s think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] Â«[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.Â»\\n[2] Â«Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.Â»\\n[3] Â«A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.Â»\\n[4] Â«![Cross-Encoder](./img/cross-encoder.png)\\n\\n*Figure 3 - Representation of a Cross-Encoder model*\\n\\n\\nIf a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application â with thousands or millions of objects â this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\\n\\nWe can combine the two methods to benefit from the strong points of both models! I\\'d like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.Â»\\n[5] Â«To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\\n\\nWe can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.Â»\\n\\nAssessment Criterion: Please rate how well this answer addresses the question, `What do cross encoders do?` based on the context. `They index data.`\\n\\nAssessed Answer: They index data.\\n\\nReasoning: Let\\'s think step by step in order to Assessment Answer: 1\\n\\nAssessment Answer:\\x1b[32m 1\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricLM.inspect_history(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
